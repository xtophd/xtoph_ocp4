- delegate_to: localhost 
  run_once: true
  block:

      #
      #    If this is an assisted-installer 
      #    we have a chance to clean up cluster configuration
      #    in the AI portal
      #

      - name: "xtoph_ocp4 : preunistall-node-shutdown : check for ocm offline token"
        stat:
          path: "./config/ocm-token.txt"
        register: token_test

      - name: "xtoph_ocp4 : preunistall-node-shutdown : load ocm offline token"
        shell: cat ./config/ocm-token.txt
        register: ocm_token
        when:  
          - token_test.stat.exists
      #
      #    Proceed with node shutdown if it is up, and wait
      #

      - name: "xtoph_ocp4 : preunistall-node-shutdown : test if bastion node is accessible"
        wait_for:
          host: "{{ xtoph_ocp4.bastion.host_fqdn }}"
          connect_timeout: 5
          port: 22
          sleep: 1
          state: started
          timeout: 10
        register: bastion_status
        ignore_errors: yes

      - name: "xtoph_ocp4 : preunistall-node-shutdown : delete cluster configuration (ignore error if it does not)"
        delegate_to: "{{ xtoph_ocp4.bastion.host_fqdn }}"
        shell:
          cmd: |
            aicli --offlinetoken {{ ocm_token.stdout|quote }}      \
                delete cluster {{ xtoph_ocp4.ocp.clustername }} -y
        ignore_errors: true
        when: 
          - token_test.stat.exists
          - bastion_status is not failed

  when:
    - xtoph_ocp4.bastion.assisted_installer == true
    - xtoph_ocp4.ocp.provisioner == "ai"



- delegate_to: localhost 
  block:

      #
      #    Remove the openshift deployment lock
      #

      - name: "xtoph_ocp4: config-bastion-ocp-unlock : remove lock ({{ workshop_extras.ocp_creds_dir }}/xtoph_ocp4_deployment.lock)"
        run_once: true
        file:
          path:  "{{ xtoph_ocp4.ocp.creds_dir }}/xtoph_ocp4_deployment.lock"
          state: absent

      #
      #    Proceed with node shutdown if it is up, and wait
      #

      - name: "xtoph_ocp4 : preunistall-node-shutdown : test existence of OCP node via ssh port"
        wait_for:
          host: "{{ inventory_hostname }}"
          connect_timeout: 5
          port: 22
          sleep: 1
          state: started
          timeout: 10
        register: ssh_status
        ignore_errors: yes


      #
      #    By specifying --force twice, systemd essentially unplugs the OS (not a safe shutdown), once is good enough
      #

      - name: "xtoph_ocp4 : preunistall-node-shutdown : issue graceful OCP node poweroff"
        delegate_to: "{{ xtoph_ocp4.bastion.host_fqdn }}"
        shell:
          cmd: |
            ssh core@{{ inventory_hostname }} sudo systemctl -f poweroff
        when: 
          - ssh_status is not failed
          - inventory_hostname in xtoph_ocp4._cluster_bootstrap + xtoph_ocp4._cluster_masters + xtoph_ocp4._cluster_workers + xtoph_ocp4._cluster_sno

      - name: "xtoph_ocp4 : preunistall-node-shutdown : wait for OCP node systemd port stopped"
        wait_for:
          host: "{{ inventory_hostname }}"
          connect_timeout: 3
          port: 111
          sleep: 5
          state: stopped
          timeout: 180
        ignore_errors: yes
        when: 
          - ssh_status is not failed
          - inventory_hostname in xtoph_ocp4._cluster_bootstrap + xtoph_ocp4._cluster_masters + xtoph_ocp4._cluster_workers + xtoph_ocp4._cluster_sno

  rescue:
      - debug: msg="node {{inventory_hostname}} is already down"

